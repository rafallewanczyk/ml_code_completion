%\vspace{-5pt}
\section{Conclusions and Future Work}
\label{sec:conclusions}

\noindent
Among the methods we tried so far, chaining two matrix-vector models with a
relu nonlinearity performs the best.
This is also the most general of all models we tried, so
the higher accuracy is not surprising. Given the results for correlation between
input and target tokens, we would like to experiment with larger windows
where we exclude odd tokens.

The word vector algorithms we tried so far do not capture file specific
patterns. Non-key tokens that are local to the file, and outside the window are
ignored. We want to try out a recurrent neural network (RNN)
approach~\cite{rnn} or a long short-term memory (LSTM) approach~\cite{lstm,
rnnlstm} to
capture file specific patterns. Another approach that we are considering is to
use file-specific word vectors --- each token in a file is a linear
transformation of its \emph{base value}, which is common across all files, and
file-specific vectors corresponding to $W$ tokens preceding it.
To predict a token, we then perform a linear transformation of file-specific
vectors corresponding to last $W$ tokens, and look for the closest
file-specific token using cosine similarity. The intuition behind this approach
is to capture local context by expressing tokens as a function of the enclosing
context. This approach resembles RNNs in some ways, and requires a forward and
a backward pass to update file-specific vectors and cost function.
File-specific vectors can be thought of as the hidden state in RNNs, but
instead of using hidden state from just the previous stage, we now use
hidden state from previous $W$ stages.

Finally, time permitting,
we would like to pick the two best algorithms, train them on different
window sizes $W$, assign a confidence to the prediction from each algorithm and
window size, and pick the prediction with the highest score.
The intuition here is that some tokens might be predicted well by
looking at a large window of preceding tokens, while some other tokens might
be predicted well by looking at a few preceding tokens (such as {\tt for}
statements).
