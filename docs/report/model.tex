%\vspace{-5pt}
\section{Modeling Tokens}
\label{sec:model}

\noindent
One of our objectives of learning based code prediction is to do away with the
tedious process of building grammar based rules for different languages.
We treat codes in the training set in a language agnostic way. The first step
is to build a dictionary of tokens or words that can occur in the code, that we
can take as input to predict the next token. We build this dictionary by
reading all code, and treating each consecutive set of alphanumeric characters
and (\_), or each consecutive set of special characters other than alphanumeric,
(\_), space and newline as one token.  Thus, for example, the code {\tt for
(my\_var = 0; my\_var < foo; my\_var++) \{} will be tokenized into eleven
tokens: {\tt for}, {\tt my\_var}, {\tt =}, {\tt 0}, {\tt ;}, {\tt my\_var}, {\tt
<}, {\tt foo}, {\tt;}, {\tt my\_var}, {\tt++) \{}.  Given a sequence of tokens, we
then want to predict the next token. Note that these tokens do not correspond
exactly to language level tokens, e.g., {\tt++) \{} is a single token despite
containing three different language level tokens {\tt++}, {\tt)}, and
{\tt\{}.

The dictionary of tokens constructed as above is not complete, since new code
may contain new tokens that are not present in the training data. Moreover,
many of the tokens in the training data may be specific to a few files and may
never occur again. To address these issues, we divide the tokens into two
categories:

a) {\it Key tokens}: A subset of $K$ most frequent tokens are categorized as
{\it key tokens}. Not surprisingly, many of
these frequent tokens are keywords for the language the project is written in,
or words that tend to occur often in that
particular project.
For example, some of the frequent tokens in Linux kernel are:
\texttt{struct}, \texttt{;}, and \texttt{dev}, out of which the first and second
are keywords in C, and the third is a token frequently used to denote device
objects in Linux. Note that while several language keywords do end up being part
of the set of key tokens, we {\it do not} manually curate the list of key tokens
to ensure that they contain only language specific keywords.

b) {\it Positional Tokens}: While key tokens occurrences constitute a major
fraction of all token occurrences ($\approx$ 60\% with 2000 key tokens),
the remainder of tokens are rarely
seen (such as names of variables, macros etc.). However, we would like our
learning algorithm to autocomplete new variable names, once they occur in a
file. For
example, given many examples of the form {\tt for (int TOKEN = 0; TOKEN < n;
TOKEN++)} (all with different values of {\tt TOKEN}), our algorithm should be
able to autocomplete {\tt for (int myIterName = 0;} with {\tt myIterName}. This
capability can not be achieved by merely trying to autocomplete between a set of
pre-defined key tokens. Hence, we also define {\it positional tokens} as follows.

Given a sequence of tokens --- an incomplete piece of code, we replace each
token which is not a key token by a string of the form {\tt
POS\_TOK\_ii}, where {\tt ii} is the {\it position} of that token
within that sequence. A non-keyword token which is repeated multiple times in a
sequence is assigned the index corresponding to its first appearance. For
example, given the sequence of tokens {\tt[for, int, myVarName, =, 0, ;,
myVarName, <, n, ;, myVarName, ++]}, where {\tt myVarName} is not a key token,
we construct the new sequence {\tt[for, int, POS\_TOK\_2, =, 0, ;, POS\_TOK\_2,
<, n, ;, POS\_TOK\_2, ++]}. In case the token to be predicted is not a key token
but has appeared in the window, it is also replaced by the corresponding
positional token string. If the target has not appeared before in the window, it
is assumed to be a special token {\tt UNKNOWN}. However, as we describe in
Section~\ref{sec:methods}, we ignore such windows since in many such
cases the token is actually a hereto unseen token.

Such an encoding is advantageous since now the set of prediction targets to
choose from is simply the union of the key tokens and the positional token
strings (of the form {\tt POS\_TOKEN\_ii}). In case of a fixed window size of
$W$ and a fixed number of key tokens $K$, it can be seen that the total number
of prediction targets is $K+W+1$ ($K$ key tokens, $W$ positional tokens, and 1
unknown token), i.e., a {\it constant}. This immediately opens up the
possibility of applying simple models such as logistic/neural network based
classifiers.
