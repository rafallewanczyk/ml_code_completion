%\vspace{-5pt}
\section{Methods}
\label{sec:methods}

\noindent
In this model, we simply take the set of $K+W$ positional and key tokens, and
the set of $K+W+1$ output tokens, and fit a model similar to word
vectors~\cite{ref:mikolov:wvec}. The details of the model are described below.

We first assign a $D$ dimensional vector to each one of $K+W$ different key
tokens and positional tokens. We denote such token-vectors by $v_i$, where $1
\leq i \leq K+W$. Next, given a fixed size window of tokens $[t_1, t_2, \ldots,
t_W]$, we compute a score for each possible output $j$, $s_j$ as a function of
the token-vectors of the tokens in the window, i.e.,
\[
s_j = f_j\left(v_{t_1}, v_{t_2}, \ldots v_{t_W}\right)
\]
The final loss function for this particular example is given by 
\[
L = \log\left(\frac{e^{s_{t_o}}}{\sum_j{e^{s_j}}}\right)
\]
where $t_o$ is the output token. This is the cross-entropy loss
between softmax based probabilities for each output and the actual observed
output. According to the actual form of the function $f_j$, we get different
models. A few of these models are described below.

For each model, we use {\sc AdaGrad} optimizer to minimize the total loss
function with respect to the parameters of that model. {\sc AdaGrad} was chosen
because it gave the best performance in our case among other alternatives such
as vanilla SGD and momentum based SGD.

\subsection{Fixed Window Weight Model}
In the spirit of continuous bag-of-word (CBOW) model~\cite{ref:mikolov:wvec}, in
this model we assume that a token at position $i$ has a ``weight'' of $w_i$, and
combine the token-vectors of the window according to these weights. The final
score is assumed to be a linear function of this weighted token-vectors. Thus,
the overall model is
\begin{align}
u &= \sum_{i=1}^{W}{w_i v_{t_i}}\\
s_j &= p_j^Tu\\
L &= \log\left(\frac{e^{s_{t_o}}}{\sum_j{e^{s_j}}}\right)
\end{align}
The parameters in this model are the weights $w_i$, the word vectors $v_i$, and
the ``prediction'' vectors $p_j$. The gradients of the loss w.r.t. the
parameters are obtained by backpropagation, the details of which are omitted
here for space.

\subsection{Matrix Vector Model}
In this case, we do not introduce any averaging as in the case of CBOW, but
instead simply concatenate the token-vectors to create a larger vector which is
then used to create the scores. Formally, the model is
\begin{align}
u &= [v_{t_1}; v_{t_2}; v_{t_3}; \ldots; v_{t_W}]\\
s_j &= p_j^Tu\\
L &= \log\left(\frac{e^{s_{t_o}}}{\sum_j{e^{s_j}}}\right)
\end{align}
Note that since here $u$ is $DW$ dimensional instead of being $D$ dimensional as
in the case of Fixed Window Weight Model. Thus, the prediction vectors $p_j$ are
much higher dimensional as well, which means this model has a higher number
of parameters as compared to Fixed Window Weight Model.

\subsection{Feed-Forward Neural Network Model}
\label{sec:nnlm}
This case differs from the Matrix Vector Model by addition of one or more
non-linear transformations between the concatenated token-vectors and the final
scores. We denote a specific instance of this model by NL-$k$, where $k$ is the
number of non-linearity layers it contains. Thus, for example, NL-$0$ is
equivalent to the Matrix-Vector model described above, while for NL-$3$ the
probabilities are obtained as:
\begin{align}
u &= [v_{t_1}; v_{t_2}; v_{t_3}; \ldots; v_{t_W}]\\
z_1 &= \text{relu}(Q_1u)\\
z_2 &= \text{relu}(Q_2z_1)\\
z_3 &= \text{relu}(Q_3z_2)\\
s_j &= p_j^Tz_3\\
L &= \log\left(\frac{e^{s_{t_o}}}{\sum_j{e^{s_j}}}\right)
\end{align}
Here $\text{relu}(.)$ is the rectified linear unit, i.e., $\text{relu}(x) = x$
if $x \geq 0$, and $0$ otherwise. In this work, we specifically experimented
with NL-$0$, NL-$1$, NL-$2$, and NL-$3$.

\subsection{Feed-Forward Model with Soft Attention}
\label{sec:annlm}
Motivated by the recent successes of attention based models, we explore an
attention mechanism for our problem as well. We experiment with a
``soft''-attention model, i.e., a weight $a_i$ between 0 and 1 is assigned to
each position $i$. The attentions are assumed to be the result of a sigmoid
function on a linear combination of the concatenated word-vectors. Subsequently,
the word vector for the $i^{th}$ token is weighted by $a_i$, and the
aforementioned NL-$k$ model is applied on the concatenation of these weighted
word vectors instead. Thus, for example, an attention based NL-$3$ model takes
the form of:
\begin{align}
u &= [v_{t_1}; v_{t_2}; v_{t_3}; \ldots; v_{t_W}]\\
a &= \sigma(Au)\\
z &= [a_1v_{t_1}; a_2v_{t_2}; a_3v_{t_3}; \ldots; a_Wv_{t_W}]\\
s_j &= \text{NL-}3(z ; Q_1, Q_2, Q_3, p_j)\\
L &= \log\left(\frac{e^{s_{t_o}}}{\sum_j{e^{s_j}}}\right)
\end{align}
Here NL-$3(x ; Q_1, Q_2, Q_3, p_j)$ is the function going from $u$ to $s_j$ in
the case of a NL-$3$ model, described in Section~\ref{sec:nnlm}.

\subsection{GRU based Recurrent Model}
\label{sec:rnnlm}
Recurrent neural network models based on cells like LSTM~\cite{ref:lstm} and
GRU~\cite{ref:gru} have recently been shown to achieve state-of-the-art
performance in language models. Inspired by these results, we also attempted to
use a GRU based recurrent model for our prediction task. Specifically, our model
was the following:
\begin{align}
[g_1; g_2; g_3; \ldots; g_{W}] &= \text{GRU}([v_{t_1}; v_{t_2}; v_{t_3}; \ldots;
v_{t_W}])\\
s_j &= p_j^T[g]\\
L &= \log\left(\frac{e^{s_{t_o}}}{\sum_j{e^{s_j}}}\right)
\end{align}
Here, $g_i$ is the output of the $i^{th}$ GRU cell, and we have a dense layer
after that to get the scores for each output token. As we did not achieve
competitive performance with this model as compared to NL-$1$, we did not
experiment further with deeper GRU models.
