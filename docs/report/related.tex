%\vspace{-5pt}
\section{Related Work}
\label{sec:related}

We use natural language techniques for predicting code.
In this section, we review some literature on language models and
neural network based language models.

\subsection{Traditional Language Models}
Statistical models of languages have been used extensively in various natural
language processing tasks. One of the simplest such models proposed is the
$n$-gram model, where the frequencies of consecutive $n$ tokens are used to
predict the probability of occurence of a sequence of tokens. The frequencies of
such $n$-grams are obtained from a corpus, and are smoothed by various
algorithms such as Kneser-Ney smoothing~\cite{ref:kneser-ney} to account for
sparsity of $n$-grams. Such models can be easily extended to code completion,
by modeling each token as a word and doing $n$-gram analysis over a
codebase. However, such models do not capture
the high fraction of unknown tokens from variable and function names very well.
%used for code completion considering the high fraction of {\tt UNKNOWN} tokens
%in codes. For example, even in a simple case such as {\tt for(int SOMEVAR = 0;
%SOMEVAR < EXPRESSION; SOMEVAR++)}, the token {\tt SOMEVAR} can be arbitrary, and
%{\tt EXPRESSION} can consist of multiple tokens. Also, it has been shown that
%neural network based language models far outperform such traditional language
%models. Hence, in our work, we only consider neural network based language
%models.

\subsection{Word Vector Embeddings and Neural Network Language Models (NNLM)}
In their seminal work, Bengio et al.~\cite{ref:embedding} first propose a
vector embedding space for words for constructing a language model,
that significantly outperforms traditional $n$-gram based
approaches. Their proposal maps each word to a vector and expresses the
probability of a token as the result of a multi-layer neural network on the
vectors of a limited window of neighboring words. T. Mikolov
et al.~ \cite{ref:regularities} show that such models can capture word-pair relationships
(such as ``{\em king} is to {\em queen} as {\em man} is to {\em woman}'') in
the form of vector operations (i.e., $v_{\text king} -
v_{\text queen} = v_{\text man} - v_{\text woman}$). To distinguish such models
from {\it recurrent} models, we
call them {\it feed-forward} neural network language models.
We use the same concept in our token-embedding. We found singificant clustering
of similar tokens, such as {\tt uint8\_t} and {\tt uint16\_t}, but were unable
to fund vector relationships.
%We use the same concept in our token-embeddings, where we attempt to predict
%probabilities of the next token depending on the token-vectors of a fixed window
%of prior tokens.
%We were able to observe significant clustering in the word
%vectors of similar tokens (e.g., vectors of {\tt uint8\_t} and {\tt uint16\_t}
%are very similar). However, we were {\it unable} to find vector relationships
%between tokens such as those discussed above.

Later, such feed-forward NNLMs have been extended by Mikolov et
al.~\cite{ref:mikolov:wvec} where they propose the continuous
bag-of-words (CBOW), and the skip-gram. In CBOW, the probability of a token is
obtained from the average of vectors of the neighboring tokens, whereas in
skip-gram model the probabilities of the neighboring tokens is obtained from the
vector of a single token. The main advantage of such models is their
simplicity, which enables them to train
% relative to a multi-layer feed-forward network, which enables them to be trained
faster on billion-word corpuses. However, in our case, the size of data was not
so huge, and we could train even a 4-layer model in 3 days.

\subsection{RNN Language Models}
\label{sec:rel:rnnlm}
A significant drawback of feed-forward neural network based language models is
their inability to consider dependencies longer than the window size. To fix
this shortcoming, Mikolov et al. proposed a recurrent neural network based
language model~\cite{ref:rnnlm}, which associates a cell with a hidden state
at each position, and updates the state with each token. Subsequently, more
advanced memory cells such as LSTM~\cite{ref:lstm} and GRU~\cite{ref:gru} have
been used for language modeling tasks. More sophsticated models such as
tree-based LSTMs ~\cite{ref:treelstm} have also
been proposed. We experimented with using GRUs in our setup, but surprisingly
did not find them to be competitive with feed-forward networks.

\subsection{Attention Based Models}
\label{sec:attn}
Recently, neural network models with {\it attention}, i.e., models that weigh
different parts of the input differently have received a lot of attention (pun
intended) in areas such as image captioning~\cite{ref:showattendtell} and
language translation~\cite{ref:nmt,ref:nmt2}. In such models, the actual task of
prediction is separated into two parts: the attention mechanism which
``selects'' the part of input that is important, and the actual prediction
mechanism which predicts the output given the weighted input. We found an
attention based feed-forward model to be the best performing model among the
ones we considered.

\subsection{Code Completion}
\label{sec:code-completion}
Frequency, association and matching neighbor based
approaches~\cite{ref:learningexamples} are used to improve predictions of IDEs such
as Eclipse.
Our learning approach, on the other hand, attempts to automatically
learn such rules and patterns.
